# -*- coding: utf-8 -*-
"""House Price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uPSULECkPfjxEnlkhebhBDDhnK3tSIx_

**House Price Prediction with Ames Housing Dataset**

Import all the required libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""Load the Dataset"""

df = pd.read_csv('/content/AmesHousing.csv')

df

# see columns
df.columns

# see column types
df.info()

# describe the dataset
df.describe()

# see missing values
df.isnull().sum()

# see total number of missing values in dataset
missing  = df.isnull().sum()
print(missing[missing>0])

# count no.of columns in the missing values
missing  = df.isnull().sum()
print(missing[missing>0].count())

"""Filling numerical values with the median helps mitigate the influence of outliers and preserves the distribution of the data, while filling categorical values with the mode maintains the most common category and improves data integrity. These techniques are effective and widely used in practice, helping to create a more robust dataset for machine learning models."""

# Fill missing values for numerical features
df['Lot Frontage'].fillna(df['Lot Frontage'].median(), inplace=True)
df['Mas Vnr Area'].fillna(df['Mas Vnr Area'].median(), inplace=True)
df['BsmtFin SF 1'].fillna(0, inplace=True)
df['BsmtFin SF 2'].fillna(0, inplace=True)
df['Bsmt Unf SF'].fillna(0, inplace=True)
df['Garage Cars'].fillna(0, inplace=True)
df['Garage Area'].fillna(0, inplace=True)
df['Total Bsmt SF'].fillna(0, inplace=True)
df['Bsmt Full Bath'].fillna(0, inplace=True)
df['Bsmt Half Bath'].fillna(0, inplace=True)
df['Garage Yr Blt'].fillna(0, inplace=True)


# Fill missing values for categorical features
df['Alley'].fillna('None', inplace=True)
df['Bsmt Qual'].fillna('None', inplace=True)
df['Bsmt Cond'].fillna('None', inplace=True)
df['Bsmt Exposure'].fillna('None', inplace=True)
df['BsmtFin Type 1'].fillna('None', inplace=True)
df['BsmtFin Type 2'].fillna('None', inplace=True)
df['Electrical'].fillna(df['Electrical'].mode()[0], inplace=True)
df['Fireplace Qu'].fillna('None', inplace=True)
df['Garage Type'].fillna('None', inplace=True)
df['Garage Finish'].fillna('None', inplace=True)
df['Garage Qual'].fillna('None', inplace=True)
df['Garage Cond'].fillna('None', inplace=True)
df['Pool QC'].fillna('None', inplace=True)
df['Fence'].fillna('None', inplace=True)
df['Misc Feature'].fillna('None', inplace=True)

# Drop features with too many missing values
df.drop(columns=['Pool QC', 'Misc Feature', 'Alley', 'Fence', 'Fireplace Qu', 'Mas Vnr Type'], inplace=True)

miss = df.isnull().sum()
print(miss[miss>0].count())

df.head(5)

df['MS Zoning']

df.columns

# viewing caterogical, continuous columns
import pandas as pd

# Assuming 'df' is your DataFrame
def classify_columns(df):
    categorical_cols = []
    continuous_cols = []

    for column in df.columns:
        if pd.api.types.is_categorical_dtype(df[column]) or pd.api.types.is_object_dtype(df[column]):
            categorical_cols.append(column)
        elif pd.api.types.is_numeric_dtype(df[column]):
            continuous_cols.append(column)

    return categorical_cols, continuous_cols

# Get the classification
categorical_columns, continuous_columns = classify_columns(df)

# Display the results
print("Categorical columns:", categorical_columns)
print("Continuous columns:", continuous_columns)

# viewing caterogical, continuous columns
import pandas as pd

# Assuming 'df' is your DataFrame
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
continuous_cols = df.select_dtypes(include=['number']).columns.tolist()

# Display the results
print("Categorical columns:", categorical_cols)
print("Continuous columns:", continuous_cols)

# count categorical and continuous columns
num_categorical = len(categorical_cols)
print(f'categorical values: {num_categorical}')
num_continuous = len(continuous_cols)
print(f'continuous values: {num_continuous}')

"""Working on categorical columns"""

df.head(5)

# one-hot encoding
import pandas as pd

# Load your dataset (assuming it's in CSV format)
# df = pd.read_csv('path_to_your_ames_housing_dataset.csv')

# Automatically detect and encode all categorical variables
df = pd.get_dummies(df, drop_first=True)

# Display the first few rows of the encoded DataFrame
print(df.head())

# Check the shape of the encoded DataFrame
print("Encoded dataset shape:", df.shape)

# Display all columns
pd.set_option('display.max_columns', None)  # Show all columns
print(df.head())

df

df = pd.get_dummies(df, drop_first=True, dtype=int)

print(df.dtypes.value_counts())  # Check data types

df = df.astype({col: int for col in df.select_dtypes(include='bool').columns})

df

df.dtypes.value_counts()

"""**Check if the data has outliers**"""

# Summary Statistics (IQR Method)

# Identify numerical columns only
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Calculate IQR
Q1 = df[num_cols].quantile(0.25)
Q3 = df[num_cols].quantile(0.75)
IQR = Q3 - Q1

# Detect outliers
outliers = ((df[num_cols] < (Q1 - 1.5 * IQR)) | (df[num_cols] > (Q3 + 1.5 * IQR))).sum()

# Display number of outliers per column
print(outliers[outliers > 0].sort_values(ascending=False))

# Box plots
# Plot boxplots for numerical columns with outliers
outlier_cols = outliers[outliers > 0].index

plt.figure(figsize=(12, 6))
df[outlier_cols].boxplot(rot=90)  # Rotate labels for better readability
plt.title("Boxplot of Numerical Features with Outliers")
plt.show()

# Filter only continuous numerical columns (not one-hot encoded binary variables)
num_cols = df.select_dtypes(include=['int64', 'float64'])

# Remove one-hot encoded binary columns (only keep those with more than 2 unique values)
continuous_cols = [col for col in num_cols if df[col].nunique() > 2]

# Calculate IQR for continuous variables only
Q1 = df[continuous_cols].quantile(0.25)
Q3 = df[continuous_cols].quantile(0.75)
IQR = Q3 - Q1

# Detect outliers
outliers = ((df[continuous_cols] < (Q1 - 1.5 * IQR)) | (df[continuous_cols] > (Q3 + 1.5 * IQR))).sum()

# Display outliers for continuous features
print(outliers[outliers > 0].sort_values(ascending=False))

# Remove outliers
# Define outlier threshold using IQR
df_no_outliers = df.copy()
for col in outliers.index:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Remove rows where values are extreme
    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower_bound) & (df_no_outliers[col] <= upper_bound)]

print("New dataset shape:", df_no_outliers.shape)

# Cap outliers using 1st and 99th percentiles
for col in outliers.index:
    lower = df[col].quantile(0.01)
    upper = df[col].quantile(0.99)
    df[col] = np.clip(df[col], lower, upper)

print("Outliers capped successfully!")

df.head(5)

#checking if data is normally distributed or not
import matplotlib.pyplot as plt
import seaborn as sns

# Select continuous numerical columns (ignoring one-hot encoded binary features)
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
continuous_cols = [col for col in num_cols if df[col].nunique() > 2]

# Plot histograms
df[continuous_cols].hist(figsize=(15, 12), bins=30, edgecolor='black')
plt.suptitle("Histograms of Continuous Variables", fontsize=16)
plt.show()



"""Feature Scaling"""

# After addressing skewness, you may want to scale the features to ensure they are on a similar scale, especially before training models.
# Standardization (mean = 0, std = 1).
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[continuous_cols] = scaler.fit_transform(df[continuous_cols])

df

df.columns

print(df['SalePrice'].head())

"""Split the dataset into x and y"""

x = df.drop('SalePrice', axis=1)
x

y = df['SalePrice']

y



"""Split the data into train and test"""

# import libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20, random_state=42)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

#import the model
model = LinearRegression()

#Fit the model
model.fit(x_train, y_train)

# see training error
train_err = model.predict(x_train)

# see training error
#see error rate in training - M.S.E and R2
mse = mean_squared_error(y_train, train_err) # same as train_pred, y_train
print(f'Mse - {mse}')

mae = mean_absolute_error(y_train, train_err) # same as train_pred, y_train
print(f'Mae - {mae}')

from sklearn.metrics import root_mean_squared_error
rmse = root_mean_squared_error(y_train, train_err) # same as train_pred, y_train
print(f'rmse - {rmse}')

r2 = r2_score(y_train, train_err) * 100 # not same as train_pred, y_train - 62%
print(f'r2 score is {r2}')

# convert mse into percentage
# Calculate mean of the actual target values
mean_sale_price = y_train.mean()

# Calculate MSE as before
mse = mean_squared_error(y_train, train_err)

# Convert MSE to percentage of the mean sale price
mse_percentage = (mse / mean_sale_price) * 100

print("Mean Squared Error (MSE) in percentage:", mse_percentage)

#plot the graphs
# training metrics
plt.scatter(y_train, train_err) #actual - xaxis and predicted - yaxis
plt.xlabel('actual')
plt.ylabel('predicted')
plt.title('actual vs prediction - training')
plt.show()

#plot the graphs
# training metrics
plt.scatter(y_train, train_err) #actual - xaxis and predicted - yaxis
plt.plot(y_train, train_err)
plt.xlabel('actual')
plt.ylabel('predicted')
plt.title('actual vs prediction - training')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_train, train_err, color='blue', alpha=0.6, label='Predictions')

# Sort the actual values and corresponding predictions for a smooth line
sorted_indices = np.argsort(y_train)
plt.plot(y_train.iloc[sorted_indices], train_err[sorted_indices], color='orange', label='Model Line')

# Plot a line for perfect predictions
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--', label='Perfect Prediction')

# Labels and Title
plt.xlabel('Actual Sale Prices')
plt.ylabel('Predicted Sale Prices')
plt.title('Actual vs. Predicted Sale Prices (Training Set)')
plt.legend()
plt.grid()
plt.show()

# test predictions
y_pred = model.predict(x_test)

# see training error
#see error rate in training - M.S.E and R2
mse = mean_squared_error(y_test, y_pred) # same as train_pred, y_train
print(f'Mse - {mse}')

mae = mean_absolute_error(y_test, y_pred) # same as train_pred, y_train
print(f'Mae - {mae}')

from sklearn.metrics import root_mean_squared_error
rmse = root_mean_squared_error(y_test, y_pred) # same as train_pred, y_train
print(f'rmse - {rmse}')

r2 = r2_score(y_test, y_pred) * 100 # not same as train_pred, y_train - 62%
print(f'r2 score is {r2}')

import matplotlib.pyplot as plt
import numpy as np

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.6, label='Predictions')

# Plot a line for perfect predictions
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction')

# Labels and Title
plt.xlabel('Actual Sale Prices')
plt.ylabel('Predicted Sale Prices')
plt.title('Actual vs. Predicted Sale Prices (Training Set)')
plt.legend()
plt.grid()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.6, label='Predictions')

# Plot the model line for predictions
plt.plot(y_test, y_pred, color='orange', label='Model Prediction Line')

# Labels and Title
plt.xlabel('Actual Sale Prices')
plt.ylabel('Predicted Sale Prices')
plt.title('Actual vs. Predicted Sale Prices (Testing Set)')
plt.legend()
plt.grid()
plt.show()



"""Ploynomial line"""

#implementing polynomial regression
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

polynomial_reg = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())

polynomial_reg.fit(x_train, y_train)

#training error
ptrain_err = polynomial_reg.predict(x_train)

# see training error
#see error rate in training - M.S.E and R2
mse = mean_squared_error(y_train, ptrain_err) # same as train_pred, y_train
print(f'Mse - {mse}')

mae = mean_absolute_error(y_train, ptrain_err) # same as train_pred, y_train
print(f'Mae - {mae}')

from sklearn.metrics import root_mean_squared_error
rmse = root_mean_squared_error(y_train, ptrain_err) # same as train_pred, y_train
print(f'rmse - {rmse}')

r2 = r2_score(y_train, ptrain_err) * 100 # not same as train_pred, y_train - 62%
print(f'r2 score is {r2}')

import matplotlib.pyplot as plt
import numpy as np

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_train, ptrain_err, color='blue', alpha=0.6, label='Predictions')

# Sort the actual values and corresponding predictions for a smooth line
sorted_indices = np.argsort(y_train)
plt.plot(y_train.iloc[sorted_indices], ptrain_err[sorted_indices], color='orange', label='Model Line')

# Plot a line for perfect predictions
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--', label='Perfect Prediction')

# Labels and Title
plt.xlabel('Actual Sale Prices')
plt.ylabel('Predicted Sale Prices')
plt.title('Actual vs. Predicted Sale Prices (Training Set)')
plt.legend()
plt.grid()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_train, ptrain_err, color='blue', alpha=0.6, label='Predictions')

# Plot the model line for predictions
plt.plot(y_train, ptrain_err, color='orange', label='Model Prediction Line')

# Labels and Title
plt.xlabel('Actual Sale Prices')
plt.ylabel('Predicted Sale Prices')
plt.title('Actual vs. Predicted Sale Prices (Testing Set)')
plt.legend()
plt.grid()
plt.show()

#test error
py_pred = polynomial_reg.predict(x_test)

# see training error
#see error rate in training - M.S.E and R2
mse = mean_squared_error(y_test, py_pred) # same as train_pred, y_train
print(f'Mse - {mse}')

mae = mean_absolute_error(y_test, py_pred) # same as train_pred, y_train
print(f'Mae - {mae}')

from sklearn.metrics import root_mean_squared_error
rmse = root_mean_squared_error(y_test, py_pred) # same as train_pred, y_train
print(f'rmse - {rmse}')

r2 = r2_score(y_test, py_pred) * 100 # not same as train_pred, y_train - 62%
print(f'r2 score is {r2}')

#visualize the test results
plt.scatter(y_test, y_pred, color='blue')
plt.plot(y_test, y_pred, color='green')



"""Will try all the other algorithms - K-Nearest Neighbor, SVM (support vector machine), Decision Tree, Random Forest"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

a = KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')
b = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')
c = DecisionTreeRegressor(criterion='squared_error', splitter='best',max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=42)
d = RandomForestRegressor(n_estimators=200, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, random_state=42)

# Fit all the models at once to see their accuracies
models = [a, b, c, d]
model_names = ['KNN - Regression', 'SVM - Regression', 'Decision Tree', 'Random Forest']

# store results in a dictionary
results = {}

# Do all training, testing at once
for model, names in zip(models, model_names):

  #Fit the model here
  model.fit(x_train, y_train)

  #Make Predictions
  train_pred = model.predict(x_train)
  test_pred = model.predict(x_test)

  #Calculate training error
  train_mse = mean_squared_error(y_train, train_pred)
  train_mae = mean_absolute_error(y_train, train_pred)
  train_rmse = np.sqrt(train_mse)
  train_r2 = r2_score(y_train, train_pred)

  #calculate test error
  test_mse = mean_squared_error(y_test, test_pred)
  test_mae = mean_absolute_error(y_test, test_pred)
  test_rmse = np.sqrt(test_mse)
  test_r2 = r2_score(y_test, test_pred)

  #print results
  print(f' Model -- {names}')
  print(f' Training results -- mse - {train_mse:.4f}, mae - {train_mae:.4f}, rmse - {train_rmse:.4f}, r2 - {train_r2:.4f}')
  print(f' Testing results -- mse - {test_mse:.4f}, mae - {test_mae:.4f}, rmse - {test_rmse:.4f}, r2 - {test_r2:.4f}')
  print("-" * 80)



"""Best Model: SVM Regression (High test accuracy, low error)

Second Best: Random Forest (Slightly lower performance but stable)

Worst Model: Decision Tree (Extreme overfitting)

KNN: Performs decently but isn’t as strong as SVM or Random Forest.

Pruning a Decision Tree to Prevent Overfitting
Pruning helps to reduce the complexity of a decision tree and improve its generalization on unseen data.

There are two types of pruning techniques:

Pre-pruning (Stopping Criteria) - Limits the depth of the tree before it grows too complex.
Controls splits using parameters like max_depth, min_samples_split, and min_samples_leaf.

Post-pruning (Cost Complexity Pruning) -
Grows the full tree first, then removes branches that do not contribute significantly to accuracy.
Uses ccp_alpha (Cost Complexity Pruning parameter).
"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize Decision Tree with Pre-pruning parameters
dt = DecisionTreeRegressor(
    max_depth=15,  # Limit tree depth
    min_samples_split=4,  # Minimum samples needed to split a node
    min_samples_leaf=3,  # Minimum samples in leaf nodes
    max_features="sqrt",  # Consider sqrt of features at each split
    random_state=42
)

# Train the model
dt.fit(x_train, y_train)

# Predict
y_train_pred = dt.predict(x_train)
y_test_pred = dt.predict(x_test)

# Evaluate performance
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Testing MSE: {test_mse:.4f}, R²: {test_r2:.4f}")

"""Trying Gradient Descent Optimizer"""

from sklearn import ensemble

from ast import Sub
gda = ensemble.GradientBoostingRegressor(n_estimators=500, max_depth=4, subsample = 0.8, min_samples_split=4, learning_rate=0.05, loss='squared_error')

gda.fit(x_train, y_train)

#Training error
training_error = gda.predict(x_train)
#Testing error
test_error = gda.predict(x_test)

# Training accuracies
mse = mean_squared_error(y_train, training_error)
print(f'mse - {mse}')
mae = mean_absolute_error(y_train, training_error)
print(f'mae - {mae}')
rmse = root_mean_squared_error(y_train, training_error)
print(f'rmse - {rmse}')
r2 = r2_score(y_train, training_error) *100
print(f'r2 - {r2}')

#Testing accuracies
mse = mean_squared_error(y_test, test_error)
print(f'mse - {mse}')
mae = mean_absolute_error(y_test, test_error)
print(f'mae - {mae}')
rmse = root_mean_squared_error(y_test, test_error)
print(f'rmse - {rmse}')
r2 = r2_score(y_test, test_error) *100
print(f'r2 - {r2}')

